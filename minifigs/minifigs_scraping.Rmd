---
title: "Minifigs Scraping"
author: "Rose Porta"
date: "5/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
```

# brickset.com

```{r}
# check permission
robotstxt::paths_allowed(
  paths = c("https://brickset.com/minifigs/category-Adventurers")
)
```

```{r}
# read in webpage
minifigs_adventurers <-
  rvest::read_html("https://brickset.com/minifigs/category-Adventurers")
```

Note-- figure out how to replace missing values with NAs
```{r}
name <- minifigs_adventurers |>
  html_elements(".tags") |>
  html_element(".name") |>
  html_text()
name
```

```{r}
year <- minifigs_adventurers |>
  html_elements(".tags") |>
  html_element(".year") |>
  html_text() |>
  lubridate::as_date(format = "%Y") |>
  lubridate::year()
year
```

```{r}
value_used <- minifigs_adventurers |>
  html_elements("dd:nth-child(4) .plain") |>
  html_text() |>
  str_sub(start = 3) |>
  as.numeric()
value_used
```

```{r}
value_new <- minifigs_adventurers |>
  html_elements(".set:nth-child(1) .plain , .set:nth-child(2) .plain, dd:nth-child(4) .plain") |>
  html_text() |>
  str_sub(start = 3) |>
  as.numeric()
value_new
```


# bricklink.com

```{r}
# function to scrape data from one category webpage
scrape_minifigs_data <- function(url, category) {
  if (!robotstxt::paths_allowed(paths = c(url))) stop("scraping not allowed, cannot proceed")

  webpage <- rvest::read_html(url)

  if (category %in% c("Discovery", "Quatro", "Universe", "Fusion")) {
    item_number <- webpage |>
      rvest::html_elements("#id_divBlock_Main td span span") |>
      rvest::html_text()
    description <- webpage |>
      rvest::html_elements("#item-name-title") |>
      rvest::html_text()
  } else {
    item_number <- webpage |>
      rvest::html_elements("font:nth-child(1) a:nth-child(2)") |>
      rvest::html_text()
    description <- webpage |>
      rvest::html_elements("#ItemEditForm strong") |>
      rvest::html_text()
  }
  tibble(item_number, description, category = category)
}
```


```{r}
# scrape category names
categories <- rvest::read_html("https://www.bricklink.com/catalogTree.asp?itemType=M") |>
  rvest::html_elements("b") |>
  rvest::html_text()
```

```{r}
# scrape links
links <- rvest::read_html("https://www.bricklink.com/catalogTree.asp?itemType=M") |>
  rvest::html_elements("tr a")
links <- links[str_detect(links, "</b>")] |>
  rvest::html_attr("href") %>%
  paste0("https://www.bricklink.com", .)
```

```{r}
## NEW get pages function-- needs some work to get it to fully work, but prob better than 
# previous one if we ever want to run this again-- look in github history to find old one if 
# can't get this to work

# function to get number of pages in a category
get_pages <- function(url){
  webpage <- read_html(url)
  num_pages <- webpage |>
    html_elements("b:nth-child(3)") |> 
    html_text()
  num_pages
}
```

```{r}
pages <- map(links, get_pages)
pages <- replace(pages, 26, 21L) # Duplo
pages <- replace(pages, 81, 26L) # Starwars
pages <- replace(pages, 98, 37L) # Town
```

```{r}
# function to create a new url for each additional page beyond the first page
replace_page <- function(pg, url) {
  pg_char <- as.character(pg)
  url_split <- str_split(url, "catType")
  new_url <- paste0(url_split[[1]][[1]], "pg=", pg_char, "&catType", url_split[[1]][[2]])
}

# function to create all urls for each category
generate_page_links <- function(num_pg, url) {
  if (num_pg == 1L) {
    url_list <- as.list(url)
  } else {
    pages <- 2L:num_pg
    url_list <- map(pages, replace_page, url = url)
    url_list <- c(url, url_list)
  }
  url_list
}
```

```{r}
links_all <- map2(pages, links, generate_page_links)
```

```{r}
categories_data <- tibble(category = categories, link = links_all, num_pages = pages) |>
  tidyr::unnest(cols = c(link)) |>
  dplyr::mutate(link = as.character(link))
write.csv(categories_data, file = "categories.csv")
```

```{r}
# map function over all categories to scrape all data
# theoretically should work, but does not because of quota limits, see below
data_all <- purrr::map2(categories_data$link, categories_data$category, scrape_minifigs_data)
data_bind <- plyr::ldply(data_all, rbind)
# write.csv(data_bind, file = "minifigs_data.csv")
```


```{r}
# split categories data into subsets and run separately so that we don't max out on the quota limit
categories_data <- read.csv("categories.csv")

categories_subset <- categories_data[1:20, ]
data_sub <- purrr::map2(categories_subset$link, categories_subset$category, scrape_minifigs_data)
data_bind_s <- plyr::ldply(data_sub, rbind)

categories_subset2 <- categories_data[21:40, ]
data_sub2 <- purrr::map2(categories_subset2$link, categories_subset2$category, scrape_minifigs_data)
data_bind_s2 <- plyr::ldply(data_sub2, rbind)

categories_subset3 <- categories_data[41:60, ]
data_sub3 <- purrr::map2(categories_subset3$link, categories_subset3$category, scrape_minifigs_data)
data_bind_s3 <- plyr::ldply(data_sub3, rbind)

categories_subset4 <- categories_data[61:100, ]
data_sub4 <- purrr::map2(categories_subset4$link, categories_subset4$category, scrape_minifigs_data)
data_bind_s4 <- plyr::ldply(data_sub4, rbind)

categories_subset5 <- categories_data[101:150, ]
data_sub5 <- purrr::map2(categories_subset5$link, categories_subset5$category, scrape_minifigs_data)
data_bind_s5 <- plyr::ldply(data_sub5, rbind)

data_all_part1 <- list(data_bind_s, data_bind_s2, data_bind_s3, data_bind_s4, data_bind_s5) |>
  plyr::ldply(rbind)

write.csv(data_all_part1, file = "part1.csv")

categories_subset6 <- categories_data[151:200, ]
data_sub6 <- purrr::map2(categories_subset6$link, categories_subset6$category, scrape_minifigs_data)
data_bind_s6 <- plyr::ldply(data_sub6, rbind)

categories_subset7 <- categories_data[201:250, ]
data_sub7 <- purrr::map2(categories_subset7$link, categories_subset7$category, scrape_minifigs_data)
data_bind_s7 <- plyr::ldply(data_sub7, rbind)

data_all_part2 <- list(data_bind_s6, data_bind_s7) |>
  plyr::ldply(rbind)

write.csv(data_all_part2, file = "part2.csv")

categories_subset8 <- categories_data[251:325, ]
data_sub8 <- purrr::map2(categories_subset8$link, categories_subset8$category, scrape_minifigs_data)
data_bind_s8 <- plyr::ldply(data_sub8, rbind)

write.csv(data_bind_s8, file = "part3.csv")
```


```{r}
# combine all parts into one
part1 <- read_csv("part1.csv")
part2 <- read_csv("part2.csv")
part3 <- read_csv("part3.csv")

data_all <- list(part1, part2, part3) |>
  plyr::ldply(rbind) |>
  dplyr::select(item_number, description, category)

write.csv(data_all, file = "minifigs_data.csv")

# check that number of figures match number on website for each category
data_all_check <- data_all |>
  group_by(category) |>
  summarise(count = n())

categories_counts <- tibble(category = categories, num_pages = pages, num_figs = data_all_check$count, link = links)

write.csv(categories_counts, file = "category_counts.csv")
```

```{r}
# add links to each item to minifigs data
minifigs_data <- read_csv("minifigs_data.csv")
minifigs_data2 <- minifigs_data |>
  mutate(item_link = paste0("https://bricklink.com/v2/catalog/catalogitem.page?M=", item_number))
write_csv(minifigs_data2, "minifigs_data.csv")
```

```{r}
# function to scrape release year given item link
scrape_year <- function(url) {
  if (!robotstxt::paths_allowed(paths = c(url))) stop("scraping not allowed, cannot proceed")
  item_page <- rvest::read_html(url)
  release_year <- item_page |> 
    html_elements("#yearReleasedSec") |> 
    html_text()
  release_year
}
```

```{r}
# doesn't work bc timeout
release_years <- map(minifigs_data$item_link, scrape_year)
```

```{r}
# sample 100 rows of minifigs data
minifigs_data_sample <- dplyr::sample_n(minifigs_data, 100)

# scrape release years and add as column to sample data frame
release_year <- map(minifigs_data_sample$item_link, scrape_year)
release_year <- release_year |> 
  as.character() %>% 
  ifelse(. == "character(0)", "NA", .)

minifigs_data_sample <- minifigs_data_sample |> 
  mutate(release_year = release_year)

write_csv(minifigs_data_sample, file = "minifigs_sample.csv")
```

```{r}
categories <- read_csv(here::here("data", "categories.csv"))
categories_town <- categories |> 
  filter(category == "Town")
```

```{r}
town_links <- generate_page_links(62, "https://www.bricklink.com/catalogList.asp?catType=M&catString=67") |> 
  as.character()
town_data <- tibble(category = "Town", link = town_links, num_pages = 62L)
town_data_scraped <- purrr::map2(town_data$link, town_data$category, scrape_minifigs_data)
data_bind_town <- plyr::ldply(town_data_scraped, rbind)
write_csv(data_bind_town, file = "town_minifig.csv")
```

```{r}
# correct Town data to have all pages, save new csv
data <- read_csv(here::here("data", "minifigs_data.csv")) |> 
  filter(category != "Town") |> 
  select(-`...1`)
town_data <- read_csv(here::here("data", "town_minifig.csv")) |> 
  mutate(item_link = paste0("https://bricklink.com/v2/catalog/catalogitem.page?M=", item_number))

data_corrected <- bind_rows(data, town_data)
write_csv(data_corrected, "minifigs_data.csv")
```

```{r}
# ninjago data for cultural minifigs
data <- read_csv(here::here("data", "minifigs_data.csv"))
ninjago <- data |> 
  filter(category == "Ninjago")
pg6_8 <- ninjago[239:380,]

year <- map(pg6_8$item_link, scrape_year)
year <- year |> 
  as.character() %>% 
  ifelse(. == "character(0)", "NA", .)

pg6_8 <- pg6_8 |> 
  mutate(year = as.numeric(year), 
         culture_represented = "Japanese", 
         inspiration = "Ninjago",
         sets_link = paste0("https://www.bricklink.com/catalogItemIn.asp?M=", item_number, "&in=S"))

# function to scrape set ids given sets url
scrape_sets_id <- function(url) {
  item_page <- rvest::read_html(url)
  sets_id <- item_page |>
    html_elements("td:nth-child(3) a:nth-child(1)") |>
    html_text()
  sets_id
}

# function to scrape set description given sets url
scrape_sets_description <- function(url){
  item_page <- rvest::read_html(url)
  sets_description <- item_page |>
    html_elements("td:nth-child(4) font b") |>
    html_text()
  sets_description
}

set_id <- map(pg6_8$sets_link, scrape_sets_id)
set_description <- map(pg6_8$sets_link, scrape_sets_description)

pg6_8 <- pg6_8 |> 
  mutate(set_id = set_id, 
         set_description = set_description,
         set_id2 = map_chr(set_id, ~paste(.x, sep=", ", collapse=", ")),
         set_description2 = map_chr(set_description, ~paste(.x, sep=", ", collapse=", ")),
         image = "",
         gender = ""
         ) |> 
  select(image, item_number, description, culture_represented, set_id = set_id2, set_description = set_description2, category, gender, year, inspiration, link = item_link)
  
write_csv(pg6_8, "ninjago6-8.csv")
```

